# Обучение RL-агента в среде Acrobot-v1

Этот проект посвящён обучению агента с подкреплением в среде Acrobot-v1 из библиотеки Gymnasium. Основная идея работы - на практическом примере показать, как разные RL-алгоритмы и их гиперпараметры влияют на обучение в сложной задаче, требующей долгосрочного планирования.

В рамках проекта сравниваются алгоритмы PPO и A2C, исследуется влияние ключевых гиперпараметров, проводится статистический анализ результатов и визуализация обучения. Отдельное внимание уделено воспроизводимости экспериментов за счёт фиксированных seed и явной фиксации окружения.

Acrobot-v1 считается нетривиальной задачей для RL, так как агенту необходимо выучить точную последовательность раскачиваний, чтобы накопить энергию и достичь цели.

## Среда Acrobot-v1

В Acrobot агент управляет двойным маятником. Его цель - раскачать систему так, чтобы нижнее звено поднялось выше заданной высоты.

Состояние среды описывается шестью величинами: $\cos(\theta_1)$, $\sin(\theta_1)$, $\cos(\theta_2)$, $\sin(\theta_2)$, а также угловыми скоростями $\omega_1$ и $\omega_2$. Агент выбирает одно из трёх дискретных действий $-1$, $0$ или $+1$, которые соответствуют прикладываемому крутящему моменту. За каждый шаг начисляется награда $-1$, пока цель не достигнута. Эпизод завершается либо при успехе, либо через 500 шагов. Задача считается решённой, если средняя награда превышает $-100$ на серии из 10–20 эпизодов.

С физической точки зрения агенту необходимо научиться накапливать энергию за счёт раскачивания маятника, а не пытаться сразу «силой» поднять звено.

## Быстрый старт

Проект рассчитан на запуск в Google Colab или аналогичной среде. Достаточно Python версии 3.8 или выше и стандартной CPU-конфигурации Colab.

В репозитории находятся только ноутбук и README. Все вспомогательные файлы, папки, модели, результаты экспериментов и `requirements.txt` создаются автоматически в процессе выполнения ноутбука.

Для начала работы достаточно открыть ноутбук `RL_Acrobot_Experiments.ipynb` локально или в Google Colab и последовательно выполнить все ячейки.

## Эксперименты

Сначала проводится сравнение алгоритмов PPO и A2C. Оба алгоритма обучаются на 200 000 шагах, каждый запуск повторяется три раза для уменьшения влияния случайности, а оценка проводится по 20 эпизодам. Все эксперименты выполняются с фиксированным seed.

После выбора более успешного алгоритма исследуется влияние гиперпараметров PPO. Рассматриваются разные значения коэффициента дисконтирования `gamma` от 0.1 до 0.9999, а также коэффициента энтропии `ent_coef` от 0.0001 до 1.0. Для каждого значения выполняется несколько запусков с одинаковыми настройками окружения.

## Результаты

Эксперименты показали, что PPO существенно превосходит A2C в задаче Acrobot-v1. PPO достигает средней награды около $-87.9$ с успешностью 85 %, тогда как A2C не смог стабильно решить задачу. Статистический тест дал p-value 0.0669, что находится на границе значимости.

Для PPO оптимальными оказались параметры `gamma` $= 0.9999$ и `ent_coef` $= 0.5$. В этой конфигурации средняя награда составила $-80.9$, а успешность достигла 93.3 %. Результаты показывают, что задача требует крайне долгосрочного планирования и активного исследования среды.

В ноутбуке автоматически строятся графики сравнения алгоритмов, влияния `gamma` и `ent_coef`, а также сохраняются видео с демонстрацией поведения лучших агентов.

## Воспроизводимость и технические детали

Все эксперименты выполняются на CPU в Google Colab и занимают примерно 2–3 часа полного времени выполнения. Для воспроизводимости фиксируются seed всех генераторов случайных чисел, версии библиотек сохраняются в `requirements.txt`, который формируется внутри ноутбука. Обученные модели и результаты экспериментов также сохраняются автоматически.

В проекте используются PPO и A2C из Stable-Baselines3, среда Gymnasium, а также стандартные библиотеки для анализа данных и визуализации.

## Итоги и дальнейшие направления

Работа подтверждает, что PPO является более устойчивым и эффективным алгоритмом для сложных задач с длинным горизонтом планирования. Высокие значения `gamma` и достаточно большой коэффициент энтропии оказываются полезными, несмотря на их нетипичные значения.

В дальнейшем проект можно расширить за счёт экспериментов с архитектурой нейросети, дополнительными гиперпараметрами, reward shaping, curriculum learning или сравнением с другими алгоритмами обучения с подкреплением.
