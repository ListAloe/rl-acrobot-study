# Обучение RL-агента в среде Acrobot-v1

## Краткое описание

Этот проект посвящён обучению агента с помощью методов обучения с подкреплением для классической задачи Acrobot-v1 — управления двойным маятником. В работе сравниваются алгоритмы PPO и A2C, а также подробно исследуется влияние ключевых гиперпараметров на качество обучения.

Цель задачи — научить агента раскачивать маятник так, чтобы нижнее звено поднялось выше заданной высоты.

Используется среда Acrobot-v1 из библиотеки Gymnasium. Состояние описывается шестью величинами: синусами и косинусами двух углов, а также их угловыми скоростями. Агент выбирает одно из трёх дискретных действий -1, 0 или +1. За каждый шаг до достижения цели начисляется награда -1. Задача считается решённой, если средняя награда превышает -100 на отрезке из 10–20 эпизодов.

## Подход и эксперименты

В качестве основного алгоритма выбран PPO (Proximal Policy Optimization), так как он хорошо сочетает стабильность обучения и высокую эффективность.

Экспериментальная часть состоит из двух этапов. Сначала проводится сравнение PPO и A2C, после чего для PPO исследуется влияние коэффициента дисконтирования gamma и коэффициента энтропии ent_coef.

Все эксперименты запускаются с фиксированным seed для воспроизводимости. Каждая конфигурация обучается в течение 200 000 шагов, выполняется по три запуска, а итоговая оценка проводится на 20 эпизодах. Обучение полностью выполняется на CPU в среде Google Colab и занимает примерно 2–3 часа.

## Параметры обучения

В качестве базовых используются стандартные настройки PPO:

```python
learning_rate = 3e-4
n_steps = 2048
batch_size = 64
n_epochs = 10
gamma = 0.99
gae_lambda = 0.95
clip_range = 0.2
ent_coef = 0.01
vf_coef = 0.5
max_grad_norm = 0.5
policy_kwargs = {"net_arch": [dict(pi=[64, 64], vf=[64, 64])]}
```

Для сравнения алгоритмов A2C применяются следующие параметры:

```python
learning_rate = 7e-4
n_steps = 5
gamma = 0.99
gae_lambda = 1.0
ent_coef = 0.01
vf_coef = 0.25
max_grad_norm = 0.5
policy_kwargs = {"net_arch": [dict(pi=[64, 64], vf=[64, 64])]}
```

## Результаты

Эксперименты показали, что PPO значительно превосходит A2C. Средняя награда PPO составила -87.9 ± 6.1 при успешности 85 %, тогда как A2C практически не справился с задачей (средняя награда -396.1 ± 145.8, успешность 0 %).

При исследовании gamma выяснилось, что производительность стабильно растёт с увеличением значения коэффициента дисконтирования. Лучший результат был получен при gamma = 0.9999: средняя награда -80.9 ± 3.7 и успешность 93.3 %.

Эксперимент с ent_coef дал менее очевидный, но интересный результат. Оптимальным оказалось значение 0.5, обеспечившее среднюю награду -82.2 ± 5.0 и успешность 95 %. Значения до 0.5 работали стабильно, а ent_coef = 1.0 приводил к заметному ухудшению обучения.

Для лучших конфигураций записаны демонстрационные эпизоды. Обученные агенты стабильно решают задачу за 75–85 шагов, что подтверждает качество найденных политик.

## Анализ и выводы

Результаты подтверждают, что PPO лучше подходит для Acrobot-v1, чем A2C, благодаря более стабильным обновлениям политики. Задача явно требует долгосрочного планирования, поэтому значения gamma, близкие к 1.0, оказываются критически важными.

Неожиданно высокий оптимальный ent_coef можно объяснить сложностью поиска правильной последовательности действий: активное исследование помогает агенту избегать локальных оптимумов, а PPO остаётся устойчивым даже при высоком уровне энтропии.

## Дальнейшие направления

В качестве продолжения работы имеет смысл поэкспериментировать с архитектурой нейросети, в том числе более глубокими моделями или рекуррентными слоями. Также перспективным выглядит автоматический подбор гиперпараметров с помощью Optuna или Ray Tune, а также модификации PPO, например с ограничением по KL-дивергенции. Отдельный интерес представляет reward shaping для ускорения начального обучения.

## Запуск и воспроизводимость

В репозитории находятся только ноутбук и этот README. Все файлы, папки, модели, графики и `requirements.txt` создаются автоматически по ходу выполнения ноутбука в Colab или другой среде выполнения.

Для воспроизведения результатов достаточно открыть ноутбук и последовательно выполнить все ячейки. Фиксированный seed гарантирует воспроизводимость полученных чисел и графиков.
